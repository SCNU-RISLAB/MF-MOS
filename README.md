# MF-MOS: A Motion-Focused Model for Moving Object Segmentation

**ğŸ‰MF-MOS achieved a leading IoU of **_76.7%_** on [the MOS leaderboard of the SemanticKITTI](https://codalab.lisn.upsaclay.fr/competitions/7088) upon submission, demonstrating the current SOTA performance.**
![Leaderboard](./assets/Leaderboard.jpg)

## ğŸ“–How to use
### ğŸ“¦pretrained model
Our pretrained model (best in validation, with the IoU of **_76.12%_**) can be downloaded from [Google Drive](https://drive.google.com/file/d/1KGPwMr9v9GWdIB0zEGAJ8Wi0k3dvXbZt/view?usp=sharing)
### ğŸ“šDataset 
Download SemanticKITTI dataset from [SemanticKITTI](http://www.semantic-kitti.org/dataset.html#download) (including **Velodyne point clouds**, **calibration data** and **label data**).
#### Preprocessing
After downloading the dataset, the residual maps as the input of the model during training need to be generated.
Run [auto_gen_residual_images.py](./utils/auto_gen_residual_images.py) or [auto_gen_residual_images_mp.py](./utils/auto_gen_residual_images_mp.py)(with multiprocess),
and check that the path is correct before running.

The structure of one of the folders in the entire dataset is as follows:
```
DATAROOT
â””â”€â”€ sequences
    â”œâ”€â”€ 00
    â”‚Â Â  â”œâ”€â”€ poses.txt
    â”‚Â Â  â”œâ”€â”€ calib.txt
    â”‚Â Â  â”œâ”€â”€ times.txt
    â”‚Â Â  â”œâ”€â”€ labels
    â”‚Â Â  â”œâ”€â”€ residual_images_1
    â”‚Â Â  â”œâ”€â”€ residual_images_10
    â”‚Â Â  â”œâ”€â”€ residual_images_11
    â”‚Â Â  â”œâ”€â”€ residual_images_13
    â”‚Â Â  â”œâ”€â”€ residual_images_15
    â”‚Â Â  â”œâ”€â”€ residual_images_16
    â”‚Â Â  â”œâ”€â”€ residual_images_19
    â”‚Â Â  â”œâ”€â”€ residual_images_2
    â”‚Â Â  â”œâ”€â”€ residual_images_22
    â”‚Â Â  â”œâ”€â”€ residual_images_3
    â”‚Â Â  â”œâ”€â”€ residual_images_4
    â”‚Â Â  â”œâ”€â”€ residual_images_5
    â”‚Â Â  â”œâ”€â”€ residual_images_6
    â”‚Â Â  â”œâ”€â”€ residual_images_7
    â”‚Â Â  â”œâ”€â”€ residual_images_8
    â”‚Â Â  â”œâ”€â”€ residual_images_9
    â”‚Â Â  â””â”€â”€ velodyne
   ...
```
If you don't need to do augmentation for residual maps, you just need the folder with num [1, 2, 3, 4, 5, 6, 7, 8].

### ğŸ’¾Environment
Our environment: Ubuntu 18.04, CUDA 11.2 

Use conda to create the conda environment and activate it:
```shell
conda env create -f environment.yml
conda activate mfmos
```
#### TorchSparse
Install torchsparse which is used in [SIEM](./modules/PointRefine/spvcnn.py) using the commands:
```shell
sudo apt install libsparsehash-dev 
pip install --upgrade git+https://github.com/mit-han-lab/torchsparse.git@v1.4.0
```

### ğŸ“ˆTraining
Check the path in [dist_train.sh](./script/dist_train.sh), and run it to train:
```shell
bash script/dist_train.sh
```
You can change the number of GPUs as well as ID to suit your needs.
#### Train the SIEM
Once you have completed the first phase of training above, you can continue with SIEM training to get an improved performance.

Check the path in [train_siem.sh](./script/train_siem.sh) and run it to train the SIEM **(only available on single GPU)**:
```shell
bash script/train_siem.sh
```

### ğŸ“Validation and Evaluation
Check the path in [valid.sh](./script/valid.sh) and [evaluate.sh](./script/evaluate.sh).

Then, run them to get the predicted results and IoU in the paper separately:
```shell
bash script/valid.sh
# evaluation after validation
bash script/evaluate.sh
```
You can also use our pre-trained model which has been provided above to validate its performance.


### ğŸ‘€Visualization
#### Single-frame visualization
Check the path in [visualize.sh](./script/visualize.sh), and run it to visualize the results in 2D and 3D:
```shell
bash script/visualize.sh
```
If -p is empty: only ground truth will be visualized.

If -p set the path of predictions: both ground truth and predictions will be visualized.
![Single frame visualization](./assets/VisualizeSingleFrame.jpg)
#### Get the sequences video
Check the path in [viz_seqVideo.py](./utils/viz_seqVideo.py), and run it to visualize the entire sequence in the form of a video.


## ğŸ‘Acknowledgment
This repo is based on MotionSeg3D, LMNet... We are very grateful for their excellent work.
